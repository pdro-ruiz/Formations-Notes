# Desarrollo de soluciones con Azure OpenAI Service

> ğŸ—ï¸ **Concepto clave âœ** Azure OpenAI Service es un servicio en la nube que ofrece modelos de lenguaje generativos (como GPT-3/4, *embeddings* y DALLÂ·E) dentro de la plataforma Azure.

Azure OpenAI Service permite acceder a modelos de lenguaje avanzados de OpenAI (como GPT-3.5, GPT-4, incrustaciones de texto, DALLÂ·E, etc.) desde Azure. Estos modelos pueden **comprender, generar y manipular texto, cÃ³digo o imÃ¡genes** en base a indicaciones en lenguaje natural. Por ejemplo, ChatGPT es un bot que usa un modelo GPT-4 para procesar preguntas y **responder con texto de apariencia humana**. Al integrar estos modelos en Azure, las soluciones heredan ventajas de seguridad, escalabilidad e integraciÃ³n con otros servicios de la plataforma.

* ğŸ” *Ejemplo:* Para crear un chatbot de soporte tÃ©cnico que resuma informaciÃ³n y sugiera cÃ³digo, usarÃ­amos Azure OpenAI con prompts similares a los de ChatGPT.
* ğŸ“Œ Azure OpenAI Service se accede vÃ­a **API REST**, **SDKs** (Python, C#, etc.) o la interfaz web de **Azure AI Studio/Foundry**.

**AutoevaluaciÃ³n:**

* Â¿QuÃ© es Azure OpenAI Service y quÃ© modelos incluye?
* Â¿CÃ³mo se relaciona ChatGPT con Azure OpenAI Service?

<br><br><br>
## Acceso a Azure OpenAI Service

> ğŸ—ï¸ **Concepto clave âœ** Para usar Azure OpenAI debes crear (aprovisionar) un *recurso* en tu suscripciÃ³n de Azure.

El primer paso es **provisionar un recurso de Azure OpenAI** en Azure. Esto se puede hacer mediante el portal web de Azure o usando la CLI de Azure. Al crearlo, debes indicar la suscripciÃ³n, grupo de recursos, regiÃ³n, un nombre Ãºnico para el recurso y el plan de tarifa. Por ejemplo, con la CLI se usarÃ­a un comando como:

```bash
az cognitiveservices account create \
  -n <NombreÃšnico> \
  -g <MiGrupo> \
  -l <RegiÃ³n> \
  --kind OpenAI \
  --sku S0 \
  --subscription <MiSuscripciÃ³n>
```

Este comando crearÃ¡ un recurso de tipo OpenAI en la regiÃ³n especificada.

* ğŸ› ï¸ *Atajo de cÃ³digo:* En Python no hay una librerÃ­a especial para crear el recurso (usa CLI o Azure SDK).
* ğŸ”‘ El recurso de Azure OpenAI funcionarÃ¡ como contenedor donde implementas los modelos.

Una vez creado, podrÃ¡s acceder al servicio desde **Azure AI Studio** (portal [https://ai.azure.com](https://ai.azure.com)) o directamente desde tu cÃ³digo usando el *endpoint* y la *clave API* del recurso. Si otros usuarios lo usan, necesitan roles de OpenAI (`Usuario de OpenAI` o `Colaborador de OpenAI`).

**AutoevaluaciÃ³n:**

* Â¿QuÃ© datos pide Azure al crear un recurso de OpenAI?
* Menciona dos formas de crear un recurso de Azure OpenAI (portal vs CLI).

<br><br><br>
## Azure AI Foundry y CatÃ¡logo de modelos

> ğŸ—ï¸ **Concepto clave âœ** **Azure AI Foundry (AI Studio)** es la interfaz web donde gestionas despliegues de modelos generativos en Azure.

DespuÃ©s de crear el recurso, ingresa a **Azure AI Studio**. AhÃ­ eliges tu directorio y suscripciÃ³n, y luego el recurso de Azure OpenAI correspondiente. En la secciÃ³n **Azure OpenAI** verÃ¡s el **CatÃ¡logo de modelos**: una lista de modelos base disponibles (por ejemplo GPT-4, GPT-3.5, DALLÂ·E) que puedes seleccionar. Desde allÃ­ creas despliegues (implementaciones) de modelo base para usarlos.

ğŸ‘¥ **Roles necesarios:** Si no eres dueÃ±o del recurso, necesitas el rol **Usuario de OpenAI** (para ver recursos y probar en el playground) y **Colaborador de OpenAI** (para crear implementaciones).

**AutoevaluaciÃ³n:**

* Â¿QuÃ© es Azure AI Studio (Foundry) y para quÃ© se usa?
* Â¿QuÃ© significa que un modelo en el catÃ¡logo estÃ© en estado "Correcto"?

<br><br><br>
## Tipos de modelos disponibles

> ğŸ—ï¸ **Concepto clave âœ** Azure OpenAI ofrece modelos de lenguaje (GPT-4, GPT-3.5), incrustaciones, y modelos especializados (DALLÂ·E, Whisper, etc.).

Azure OpenAI incluye varios tipos de modelo:

* ğŸ¤– **GPT-4:** El modelo de Ãºltima generaciÃ³n para **texto y cÃ³digo**. Genera respuestas avanzadas basadas en prompts de lenguaje natural.
* ğŸ¤– **GPT-3.5 (GPT-35-turbo):** Modelo versÃ¡til para texto y cÃ³digo, optimizado para chat conversacional. Funciona bien en la mayorÃ­a de escenarios de IA generativa.
* ğŸ§® **Incrustaciones (Embeddings):** Convierte texto en vectores numÃ©ricos. Ãštil para anÃ¡lisis de similitud de texto, bÃºsqueda semÃ¡ntica, agrupamiento, etc..
* ğŸ¨ **DALLÂ·E:** Modelo para **generar imÃ¡genes** a partir de descripciones de lenguaje natural. Actualmente en vista previa, puede crear imÃ¡genes originales basadas en tu prompt.
* ğŸ™ï¸ **Whisper:** Modelo de **transcripciÃ³n de voz a texto**. Convierte grabaciones de audio en texto escrito.
* ğŸ”Š **Texto a voz:** Modelos que convierten texto en voz hablada de alta calidad.

> **Nota:** Los precios dependen del nÃºmero de tokens procesados y del tipo de modelo. TambiÃ©n ten en cuenta que **no todos los modelos** estÃ¡n disponibles en todas las regiones.

**AutoevaluaciÃ³n:**

* Â¿Para quÃ© se usan los modelos de incrustaciones (embeddings)?
* Menciona dos usos posibles de DALLÂ·E.


<br><br><br>
## ImplementaciÃ³n de modelos en Azure OpenAI

> ğŸ—ï¸ **Concepto clave âœ** Una vez seleccionado un modelo base, debes **desplegarlo (implementarlo)** en tu recurso Azure OpenAI para poder usarlo.

Antes de hacer peticiones, implementa el modelo elegido. Puedes crear varias implementaciones (copias) de un mismo modelo base, siempre que no excedas tu cuota de tokens por minuto. Hay tres mÃ©todos principales:

* ğŸŒ **Azure AI Studio (portal):** En el CatÃ¡logo de modelos, elige un modelo base y dale un nombre para desplegarlo.
* ğŸ’» **CLI de Azure:** Con comandos como `az cognitiveservices account deployment create` configuras un despliegue vÃ­a lÃ­nea de comandos. Ejemplo mÃ­nimo:

  ```bash
  az cognitiveservices account deployment create \
    -g MiGrupo \
    -n MiRecurso \
    --deployment-name MiModelo \
    --model-name gpt-35-turbo \
    --model-version "0125" \
    --model-format OpenAI \
    --sku-name "Standard" \
    --sku-capacity 1
  ```

  Este comando implementa `gpt-35-turbo` (versiÃ³n 0125) en tu recurso.
* ğŸ”— **API REST:** Puedes llamar directamente a la API REST de implementaciÃ³n para crear un despliegue desde tu aplicaciÃ³n. En la solicitud especificas el modelo base deseado (consulta la documentaciÃ³n oficial).

Cada implementaciÃ³n tiene un nombre Ãºnico (*deployment name*) que usarÃ¡s luego en las llamadas API.

**AutoevaluaciÃ³n:**

* Â¿QuÃ© informaciÃ³n necesitas proporcionar al implementar un modelo mediante CLI?
* Â¿QuÃ© ventajas tiene usar el portal (Studio) en vez de la CLI para desplegar modelos?

<br><br><br>
## Tipos de solicitudes (*prompts*) y tareas

> ğŸ—ï¸ **Concepto clave âœ** Un *prompt* o solicitud es el mensaje de entrada que das al modelo; con Ã©l defines la tarea que quieres (clasificar, generar, traducir, resumir, etc.).

Las solicitudes se pueden clasificar segÃºn la tarea deseada. Algunos ejemplos:

* ğŸ“ **ClasificaciÃ³n de contenido:** P. ej., dado un tweet, determinar si la opiniÃ³n es *positiva* o *negativa*.
* ğŸ“„ **GeneraciÃ³n de contenido:** P. ej., pedir al modelo que liste â€œFormas de viajarâ€ genera un listado de opciones.
* ğŸ’¬ **ConversaciÃ³n:** El modelo mantiene un diÃ¡logo. EnvÃ­ale un historial de chat y el siguiente turno del usuario, y responde como asistente.
* ğŸ”„ **TransformaciÃ³n o traducciÃ³n:** P. ej., convertir â€œHolaâ€ a francÃ©s produce â€œBonjourâ€.
* ğŸ“‘ **Resumen:** P. ej., pedir â€œResume este textoâ€¦â€ y el modelo devolverÃ¡ un resumen del contenido dado.
* ğŸš€ **ContinuaciÃ³n de texto:** Dado â€œUna manera de cultivar tomates es plantar semillasâ€¦â€, el modelo sigue el texto a partir de allÃ­.
* â“ **Preguntas y respuestas fÃ¡cticas:** P. ej., â€œÂ¿CuÃ¡ntas lunas tiene la Tierra?â€ produce â€œUnoâ€.

La **calidad de la respuesta** generada depende de varios factores, entre ellos:

* La forma en que diseÃ±as el prompt (cÃ³mo escribes la peticiÃ³n al modelo).
* Los parÃ¡metros del modelo (ver la secciÃ³n de abajo).
* Los datos en los que el modelo fue entrenado (o adaptados mediante *fine-tuning*).

ğŸ‘€ *Consejo:* Experimenta con diferentes prompts en el Azure AI Studio Playground antes de integrarlo en tu app.

**AutoevaluaciÃ³n:**

* Da un ejemplo de prompt para una tarea de resumen y otro para clasificaciÃ³n de sentimiento.
* Â¿QuÃ© factores afectan la calidad de la generaciÃ³n de texto?

<br><br><br>
## Puntos de conexiÃ³n y llamadas API

> ğŸ—ï¸ **Concepto clave âœ** Una vez implementado un modelo en Azure, interactÃºas con Ã©l mediante **endpoints (puntos de conexiÃ³n)**. Los principales son **ChatCompletion**, **Completions** y **Embeddings**.

Para usar el modelo desde tu aplicaciÃ³n, necesitas: el **endpoint** de tu recurso, tu **clave API** y el **nombre de la implementaciÃ³n** (deployment) que creaste. Con estos, puedes hacer solicitudes HTTP al servicio. Los endpoints mÃ¡s comunes son:

* ğŸ–¥ï¸ **Completions:** EnvÃ­as un prompt de texto y el modelo genera uno o varios *completions* (finalizaciones) de texto. Este endpoint es para modelos de GPT-3 clÃ¡sicos (no optimizados para chat).
* ğŸ’¬ **ChatCompletion:** EnvÃ­as una conversaciÃ³n en formato JSON con roles (`system`, `user`, `assistant`), y el modelo responde con el siguiente mensaje del chat. Por ejemplo, puedes inicializar la conversaciÃ³n con un *mensaje del sistema* para definir el rol del asistente, y luego enviar mensajes de usuario. Este endpoint es el mÃ¡s usado con modelos GPT-4 y GPT-3.5-turbo.
* ğŸ“ˆ **Embeddings:** EnvÃ­as texto y el modelo devuelve un vector de nÃºmeros (embedding) que representa el significado del texto. Ãšsalo para comparaciones de similitud o indexaciÃ³n semÃ¡ntica.

ğŸŒ *Ejemplo de ChatCompletion (curl):* enviar una conversaciÃ³n de entrenamiento:

```bash
curl https://<YOUR_ENDPOINT>.openai.azure.com/openai/deployments/<DEPLOY_NAME>/chat/completions?api-version=2023-03-15-preview \
  -H "Content-Type: application/json" \
  -H "api-key: <YOUR_API_KEY>" \
  -d '{
        "messages":[
          {"role": "system", "content": "Eres un asistente amigable enseÃ±ando sobre IA."},
          {"role": "user", "content": "Â¿CuÃ¡l es la Ãºltima versiÃ³n de modelos GPT en Azure?"}
        ]
      }'
```

El modelo responderÃ¡ con un JSON que incluye la respuesta del asistente.

ğŸ‘©â€ğŸ’» Para *obtener embeddings*, se hace una solicitud similar al endpoint `/embeddings`:

```bash
curl https://<YOUR_ENDPOINT>.openai.azure.com/openai/deployments/<DEPLOY_NAME>/embeddings?api-version=2022-12-01 \
  -H "Content-Type: application/json" \
  -H "api-key: <YOUR_API_KEY>" \
  -d '{"input": "Prueba de embeddings"}'
```

Se devuelve un JSON con la lista de vectores (uno por entrada enviada).

**Tabla de atajos de cÃ³digo (ejemplos mÃ­nimos):**

| Endpoint                   | Python (biblioteca OpenAI)                                                                                                                                                                                                                                                                  | Curl                                                                                 |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| **ChatCompletion** (texto) | `python\nimport openai\nopenai.api_key="<YOUR_API_KEY>"\nopenai.api_base="https://<YOUR_ENDPOINT>.openai.azure.com/"\nresp = openai.ChatCompletion.create(\n    model="<DEPLOY_NAME>",\n    messages=[{\"role\":\"user\",\"content\":\"Hola\"}]\n)\nprint(resp.choices[0].message.content)` | Como arriba en el ejemplo de *curl*.                                                 |
| **Embeddings**             | `python\nembedding = openai.Embedding.create(\n    engine=\"<DEPLOY_NAME>\",\n    input=\"El texto a vectorizar\"\n)\nvector = embedding.data[0].embedding\n`                                                                                                                               | Como arriba: endpoint `/embeddings` con el JSON `{"input":"El texto a vectorizar"}`. |

**AutoevaluaciÃ³n:**

* Â¿QuÃ© diferencia hay entre los endpoints **Completions** y **ChatCompletions**?
* Menciona quÃ© informaciÃ³n necesitas para llamar al API REST de Azure OpenAI (endpoint, clave, deployment).

<br><br><br>
## Uso del SDK de Azure OpenAI

> ğŸ—ï¸ **Concepto clave âœ** Existen SDKs especÃ­ficos (por ejemplo, la biblioteca `openai` de Python) que facilitan el uso de Azure OpenAI desde cÃ³digo.

AdemÃ¡s de las llamadas REST directas, puedes usar el SDK oficial de Python (mantenido por OpenAI) o bibliotecas para C#, JavaScript, etc. Estos SDKs envuelven las peticiones HTTP y ofrecen mÃ©todos sencillos. Para Python: instala la librerÃ­a con `pip install openai`, luego configuras el cliente con tu endpoint y clave. Ejemplo bÃ¡sico:

```python
from openai import OpenAI

client = OpenAI(azure_api_key="<YOUR_API_KEY>", azure_api_base="<YOUR_ENDPOINT>", api_type="azure")
resp = client.chat.completions.create(
    model="<DEPLOY_NAME>",
    messages=[
        {"role": "system", "content": "Eres un asistente amigable."},
        {"role": "user", "content": "Â¿QuÃ© es Azure OpenAI?"}
    ]
)
print(resp.choices[0].message.content)
```

Esta llamada es equivalente a la REST anterior, pero usando la biblioteca. El objeto de respuesta (`resp`) contendrÃ¡ la respuesta generada, el uso de tokens, etc. Desde Python o C# tambiÃ©n puedes ajustar parÃ¡metros opcionales como `temperature` o `max_tokens` al llamar al mÃ©todo.

**AutoevaluaciÃ³n:**

* Â¿CÃ³mo se inicializa el cliente de OpenAI en Python para Azure OpenAI (quÃ© datos pide)?
* Â¿QuÃ© biblioteca usas para consumir Azure OpenAI en Python, y cÃ³mo se instala?

<br><br><br>
## IngenierÃ­a de prompts (ingenierÃ­a de avisos)

> ğŸ—ï¸ **Concepto clave âœ** La **ingenierÃ­a de prompts** es el arte de redactar la solicitud (prompt) de forma clara y estructurada para obtener respuestas precisas y Ãºtiles.

La calidad de lo que recibe el modelo depende en gran parte de **cÃ³mo le preguntas**. Algunas buenas prÃ¡cticas:

* âœï¸ **Instrucciones claras y detalladas:** Explica con detalle lo que quieres. Por ejemplo, al generar la descripciÃ³n de un producto, decir *â€œescribe una descripciÃ³n detallada que mencione materiales ecolÃ³gicosâ€* produce un resultado mÃ¡s concreto que solo *â€œdescribe este productoâ€*.
* ğŸ”„ **Formato y estructura:** Puedes pedir que la salida tenga cierto formato. P. ej., â€œEscribe una tabla en Markdown con 6 animales, indicando su gÃ©nero y especieâ€ produce tablas estructuradas.
* ğŸ“Œ **Contenido principal vs complementario vs base:**   Incluye primero el *contenido principal* (lo que quieres procesar, p.ej., un texto para resumir) separado del prompt, y luego instrucciones. AÃ±ade *contenido complementario* (ej. â€œEstoy interesado en IA y fechas de webinarsâ€ en un correo) para guiar al modelo sobre quÃ© buscar. Si quieres respuestas basadas en informaciÃ³n especÃ­fica, usa *contenido de base* (por ejemplo, un artÃ­culo cientÃ­fico) para â€œanclarâ€ la respuesta a esos datos.
* ğŸ§‘â€ğŸ’¼ **Mensaje del sistema:** En `ChatCompletion`, el primer mensaje puede tener rol `system` para definir el tono o rol del asistente. P. ej., â€œActÃºa como traductor inglÃ©s-espaÃ±ol, solo traduce el textoâ€. El modelo seguirÃ¡ estas instrucciones al responder.
* ğŸ•°ï¸ **Sesgo de recencia:** En prompts largos, la informaciÃ³n al final suele influir mÃ¡s en la respuesta. Experimenta repitiendo instrucciones claves al final si es necesario. En chats, los mensajes recientes del historial afectan mÃ¡s la respuesta.
* â„¹ï¸ **Mostrar contexto con ejemplos:** Cuando pidas algo complejo, a veces es Ãºtil dar ejemplos o notas al modelo en el prompt para guiar su salida.

**Ejemplos prÃ¡cticos:**

* Pedir instrucciones claras mejorÃ³ notablemente la respuesta (segunda peticiÃ³n incluye mÃ¡s detalles):

  **Aviso simple:** â€œDescribe esta botella de aguaâ€
  **Respuesta:** texto genÃ©rico sobre botella.

  **Aviso detallado:** â€œDescribe esta botella de agua 100% reciclada, menciona colores naturales, y que cada compra elimina 10 libras de plÃ¡stico del ocÃ©ano.â€
  **Respuesta:** descripciÃ³n especÃ­fica incluyendo reciclaje y beneficios ambientales.

* Pide formato estructurado:
  Aviso: â€œEscribe una tabla de 6 animales con gÃ©nero y especie.â€
  Respuesta: tabla en Markdown con filas de animales.

**AutoevaluaciÃ³n:**

* Â¿QuÃ© contiene tÃ­picamente un *mensaje del sistema* y para quÃ© sirve?
* Explica la diferencia entre **contenido principal** y **contenido de base**.

<br><br><br>
## ParÃ¡metros del modelo

> ğŸ—ï¸ **Concepto clave âœ** Los parÃ¡metros como **`temperature`** y **`top_p`** controlan la aleatoriedad de las respuestas.

Azure OpenAI permite afinar el comportamiento del modelo vÃ­a parÃ¡metros opcionales en la llamada API:

* ğŸ² **`temperature`** (0.0 a 1.0+): Controla la creatividad. Un valor bajo (p.ej. 0.2) hace que la respuesta sea mÃ¡s **determinÃ­stica y coherente**, repitiendo respuestas comunes. Un valor alto (p.ej. 0.8) produce respuestas mÃ¡s **variadas y creativas**.
* ğŸ”¢ **`top_p`** (nuclear sampling): Ajusta la probabilidad acumulada de las palabras. Reduce el vocabulario de salida. Un valor alto tambiÃ©n aumenta la diversidad lÃ©xica.
* ğŸ’  **Tokens mÃ¡ximos (`max_tokens`)**: Limita la longitud de la respuesta.
* Otros parÃ¡metros: `frequency_penalty`, `presence_penalty`, etc., para pulir el estilo.

> *Consejo:* Ajusta `temperature` y `top_p` por separado para ver su efecto. Valores altos dan resultados mÃ¡s imaginativos pero menos enfocados.

**AutoevaluaciÃ³n:**

* Â¿QuÃ© efecto tiene aumentar la temperatura en la generaciÃ³n de texto?
* Â¿CuÃ¡ndo podrÃ­as querer un `temperature` bajo?

<br><br><br>
## Fine-tuning vs RAG (OpenAI en los datos)

> ğŸ—ï¸ **Concepto clave âœ** El **ajuste fino** (fine-tuning) entrena un modelo base con tus datos, mientras que **RAG** (Retrieval-Augmented Generation) conecta el modelo a tus datos sin entrenar.

Azure OpenAI ofrece dos estrategias para incorporar datos especÃ­ficos:

* ğŸ›  **Fine-tuning:** Reentrena un modelo bÃ¡sico existente (p.ej. GPT-35-turbo) con tu propio conjunto de entrenamiento. Esto puede mejorar las respuestas para tu caso de uso concreto y manejar contextos grandes. Sin embargo, **es costoso y consume mucho tiempo**. Se usa solo cuando es necesario un modelo altamente personalizado.

* ğŸ“š **RAG (*en los datos*):** No entrena modelo nuevo. En su lugar, cada vez que envÃ­as un prompt, primero **buscas informaciÃ³n** relevante en tus datos (por ejemplo, usando Azure Cognitive Search) y luego *inyectas* esos datos en el prompt al modelo. De esta forma el modelo genera respuestas basadas en datos actualizados sin cambiar el modelo bÃ¡sico.

Ejemplo: un asistente que responde usando documentos de tu empresa. Con RAG, Azure AI busca pasajes relacionados y los agrega al prompt antes de llamar al modelo.

**AutoevaluaciÃ³n:**

* Â¿QuÃ© ventaja tiene RAG frente al ajuste fino?
* Â¿Por quÃ© el ajuste fino solo se recomienda en casos necesarios?

<br><br><br>
## ConexiÃ³n de datos (*RAG con datos propios*)

> ğŸ—ï¸ **Concepto clave âœ** Azure AI Studio permite conectar tus propios datos (archivos, Ã­ndices de bÃºsqueda) para usarlos en prompts con Azure OpenAI.

Para usar RAG con tus datos, sigue estos pasos:

1. **Conectar datos en AI Studio:** En Azure AI Studio, ve al playground de chat y selecciona la pestaÃ±a **Agregar datos**. Sigue el asistente para cargar archivos (.md, .txt, .pdf, .docx, etc.) o conectar a un Ã­ndice de bÃºsqueda existente. El sistema crearÃ¡ un Ã­ndice semÃ¡ntico de tu contenido.
2. **Uso en el prompt:** Al chatear, el sistema busca partes relevantes de tus datos (sobre la pregunta) y las incluye en el mensaje. De este modo, cuando envÃ­as el prompt al modelo, le estÃ¡s dando contexto actualizado de tus fuentes.
3. **Llamada API con datos:** Si programas tu propia app, en la peticiÃ³n JSON incluyes un objeto `dataSources` con la direcciÃ³n de tu servicio de Azure Cognitive Search, su clave y `indexName`. Por ejemplo:

```json
{
  "dataSources": [
    {
      "type": "AzureCognitiveSearch",
      "parameters": {
        "endpoint": "<tu_search_endpoint>",
        "key": "<tu_search_key>",
        "indexName": "<tu_indice>"
      }
    }
  ],
  "messages": [
    {"role": "system", "content": "Eres un asistente de viajes."},
    {"role": "user", "content": "Quiero ir a Nueva York. Â¿DÃ³nde me quedo?"}
  ]
}
```

El servicio enviarÃ¡ esta peticiÃ³n al endpoint de chat de tu modelo. Azure buscarÃ¡ informaciÃ³n relevante en el Ã­ndice y la usarÃ¡ para mejorar la respuesta del modelo.

**Consideraciones de tokens:** Al usar RAG, se incluyen tokens del mensaje del sistema, el historial, los fragmentos recuperados de los datos, etc. Es importante **acotar la longitud** de las preguntas y del historial para no exceder el lÃ­mite del modelo. Por ejemplo, el mensaje del sistema se trunca si supera el lÃ­mite de tokens. El modelo limita su respuesta a 1500 tokens cuando usas datos propios.

**AutoevaluaciÃ³n:**

* Â¿QuÃ© tipos de archivo puedes cargar en Azure AI Studio para RAG?
* Â¿QuÃ© informaciÃ³n debes incluir en la llamada API para usar tus datos conectados con el modelo?


<br><br><br>
## GeneraciÃ³n de imÃ¡genes con Azure OpenAI (DALLÂ·E)

> ğŸ—ï¸ **Concepto clave âœ** DALLÂ·E es el modelo de Azure OpenAI que **genera imÃ¡genes** originales a partir de una descripciÃ³n en lenguaje natural.

El servicio Azure OpenAI permite usar modelos de OpenAI para grÃ¡ficos a travÃ©s de DALLÂ·E. Puedes crear ilustraciones, diseÃ±os de productos o cualquier imagen Ãºnica descrita por texto.

**Â¿QuÃ© es DALLÂ·E?** Es una red neuronal entrenada para generar imÃ¡genes a partir de texto. Por ejemplo, el prompt *â€œUna ardilla en una motocicletaâ€* podrÃ­a producir una imagen similar a la siguiente:

![Ejemplo de imagen generada: ardilla en motocicleta](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/images/dall-e-squirrel-motorcycle.png)

> **Nota:** Las imÃ¡genes creadas por DALLÂ·E son originales, no extraÃ­das de un catÃ¡logo. El modelo genera nuevas imÃ¡genes basadas en lo que ha aprendido.

Para probar DALLÂ·E en el **Azure AI Studio**, ve al Ã¡rea de juegos *ImÃ¡genes*. AhÃ­ puedes escribir prompts de texto y ajustar parÃ¡metros como **resoluciÃ³n** (1024Ã—1024, 1792Ã—1024, etc.), **estilo** (natural o vÃ­vido) y **calidad** (estÃ¡ndar o HD).

**Uso de la API REST para DALLÂ·E:** Al igual que con texto, usas una llamada POST. El JSON de solicitud incluye al menos:

* `prompt`: la descripciÃ³n de la imagen.
* `n`: cuÃ¡ntas imÃ¡genes generar (DALLÂ·E 3 admite solo 1).
* `size`: resoluciÃ³n (e.g. `"1024x1024"`).
* Opcionales: `quality` (`"standard"` o `"hd"`, por defecto estÃ¡ndar) y `style` (`"natural"` o `"vivid"`, por defecto vÃ­vido).

Ejemplo de JSON para un escudo con esmoquin:

```json
{
  "prompt": "Un tejÃ³n usando esmoquin",
  "n": 1,
  "size": "512x512",
  "quality": "hd",
  "style": "vivid"
}
```

Con DALLÂ·E 3, la respuesta llega **sÃ­ncronamente** e incluye la URL de la imagen generada. Ejemplo de respuesta:

```json
{
  "created": 1686780744,
  "data": [
    {
      "url": "<URL de la imagen generada>",
      "revised_prompt": "A badger wearing a tuxedo"
    }
  ]
}
```

Luego puedes descargar la imagen PNG usando esa URL. Por ejemplo, el prompt anterior podrÃ­a producir una imagen como la de abajo (un tejÃ³n con esmoquin):

<p align="center"> <img src="https://learn.microsoft.com/en-us/azure/cognitive-services/openai/images/dall-e-badger-tuxedo.png" alt="TejÃ³n con esmoquin generado por DALLÂ·E"> </p> 

**AutoevaluaciÃ³n:**

* Â¿QuÃ© parÃ¡metros mÃ­nimos pide la API de DALLÂ·E para generar una imagen?
* Â¿CÃ³mo difiere la respuesta de la API entre DALLÂ·E 2 y DALLÂ·E 3 segÃºn el flujo descrito?

<br><br><br>
## Mini-glosario

* **Azure OpenAI Service:** Servicio en la nube que ofrece modelos generativos de OpenAI en la plataforma Azure.
* **ImplementaciÃ³n (Deployment):** Despliegue de un modelo base en tu recurso de Azure OpenAI (por ejemplo, â€œgpt35turboâ€) para usarlo.
* **Prompt (Aviso):** La entrada de texto (instrucciÃ³n) que envÃ­as al modelo para guiar la generaciÃ³n de la respuesta.
* **ChatCompletion:** Endpoint de API donde envÃ­as un diÃ¡logo (mensajes con roles) y obtienes la siguiente respuesta conversacional.
* **Completions:** Endpoint de API donde envÃ­as texto libre y recibes texto continuado (usado con modelos GPT-3 clÃ¡sicos).
* **Embeddings (Incrustaciones):** Representaciones vectoriales de texto; Ãºtiles para medir similitud o indexar contenido.
* **Fine-tuning (Ajuste fino):** Entrenamiento adicional de un modelo base con tus datos especÃ­ficos.
* **RAG (En los datos):** TÃ©cnica que busca informaciÃ³n relevante en tus datos externos (p. ej. Azure Cognitive Search) y la inyecta en el prompt en cada peticiÃ³n.
* **Token:** Unidad bÃ¡sica de texto procesada por el modelo (por ejemplo, cada palabra o parte de palabra). Los precios de Azure OpenAI se calculan por token procesado.
* **Temperature:** ParÃ¡metro que controla aleatoriedad en la generaciÃ³n; valores mÃ¡s altos = mÃ¡s creativas/variadas las respuestas.
* **Azure AI Studio (Foundry):** Interfaz web de Azure para gestionar e implementar modelos generativos, asÃ­ como experimentar con ellos (playground).
* **Modelo base:** Un modelo preentrenado estÃ¡ndar provisto por Microsoft (GPT-35, GPT-4, DALLÂ·E, etc.) sobre el cual se basan tus implementaciones.

<br><br><br>
## Tabla resumen (Flashcards)

| Servicio/Funcionalidad          | DescripciÃ³n breve                                                                   | Â¡A memorizar!                            |
| ------------------------------- | ----------------------------------------------------------------------------------- | ---------------------------------------- |
| **Azure OpenAI Service**        | Plataforma Azure con modelos generativos de OpenAI (GPT, DALLÂ·E, etc.).             | Modelos GPT/Ey no comparten datos.       |
| **ChatCompletion**              | Endpoint para conversaciones: envÃ­a historial con roles y recibe respuesta de chat. | ChatGPT en Azure â†’ *Mensajes con roles*. |
| **Completions**                 | Endpoint para completar texto simple: envÃ­as prompt y recibe texto generado.        | Usado en modelos GPT-3 clÃ¡sicos.         |
| **Embeddings**                  | Convierte texto en vectores numÃ©ricos. Ãštil para bÃºsquedas semÃ¡nticas.              | *Texto â†’ vectores* para similitud.       |
| **DALLÂ·E**                      | Modelo para crear imÃ¡genes desde texto.                                             | Prompt de texto â†’ imagen PNG.            |
| **endpoint (punto final)**      | URL del recurso Azure OpenAI mÃ¡s ruta especÃ­fica (ej. `/chat/completions`).         | Es parte de la URL de la peticiÃ³n.       |
| **Deployment (implementaciÃ³n)** | Nombre Ãºnico dado a un modelo desplegado (ej. â€œgpt35turboâ€).                        | Lo usan en API para identificar modelo.  |
| **temperature**                 | ParÃ¡metro (0â€“1+) que ajusta creatividad: *bajo* = coherente, *alto* = creativo.     | Controla variabilidad.                   |
| **Prompt (aviso)**              | InstrucciÃ³n o entrada textual al modelo.                                            | Redactarlo claro y detallado.            |
| **Fine-tuning (ajuste fino)**   | Retraining de un modelo con datos propios.                                          | Solo si necesitas personalizaciÃ³n.       |
| **RAG (retrieval)**             | Uso de bÃºsquedas en datos propios + IA, sin entrenar modelo.                        | Injecta datos externos al prompt.        |
| **Azure AI Studio**             | Portal web para implementar modelos y probar prompts (playground).                  | Punto de partida visual.                 |
